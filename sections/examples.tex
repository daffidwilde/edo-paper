\section{Examples}\label{section:examples}

The following examples act as a form of validation for EDO, and also highlight
some of the nuances in its use. The examples will be focused around the
clustering of data and, in particular, the \(k\)-means (Lloyd's) algorithm.
Clustering was chosen as it is a well-understood problem that is easily
accessible \-- especially when restricted to two dimensions. The \(k\)-means
algorithm is an iterative, centroid-based method that aims to minimise the
``inertia'' of the current partition, \(Z = \left\{Z_1, \ldots, Z_k\right\}\),
of some dataset \(X\):

\[
    I(Z, X) := \frac{1}{|X|} \sum_{j=1}^{k} \sum_{x \in Z_j} {d(x, z_j)}^2
\]

A full statement of the algorithm is given in~\ref{appendix:kmeans}. However, it
should be clear that \(I\) may take any non-negative value.

This inertia function is often taken as the objective of the \(k\)-means
algorithm, and is used for evaluating the final clustering. This is particularly
true when the algorithm is not being considered an unsupervised classifier where
accuracy may be used~\cite{Huang1998}. With that, the first example is to use
this inertia as the fitness function in EDO.\ That is, to find datasets which
minimise \(I\).

In this example, EDO is restricted to only two-dimensional datasets, i.e.\ \(C =
\left((2, 2)\right)\). In addition to this, all columns are formed from the
uniform distribution restricted to the unit interval, \(\mathcal{U} :=
\left\{U(a, b)~|~a, b \in [0, 1]\right\}\). The remaining parameters are as
follows: \(N~=~100\), \(R~=~(3, 100)\), \(M~=~1000\), \(b~=~0.2\), \(l~=~0\),
\(p_m~=~0.01\), and shrinkage excluded. Figure~\ref{figure:inertia} shows an
example of the fitness (left) and dimension (right) progression of the
evolutionary algorithm under these conditions.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/inertia-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/inertia-nrows.pdf}
    \end{minipage}
    \caption{Progressions for fitness and dimension (number of rows) for a run
             of EDO shown at 100 epoch intervals.}\label{figure:inertia}
\end{figure}

It is clear that there is a steep learning curve here; within the first 100
generations an individual is found with a fitness of roughly \(10^{-10}\) which
cannot be improved on for the remaining 900 epochs. In fact, this individual was
found in the \(44^{th}\) epoch as can be seen in
Figure~\ref{figure:inertia-50}. The same quick convergence is seen in the number
of rows.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/inertia-fitness-50.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/inertia-nrows-50.pdf}
    \end{minipage}
    \caption{Progressions for fitness and dimension across the first 50
             epochs.}\label{figure:inertia-50}
\end{figure}

