\section{Examples}\label{section:examples}

\subsection{\(k\)-means clustering}

The following examples act as a form of validation for EDO, and also highlight
some of the nuances in its use. The examples will be focused around the
clustering of data and, in particular, the \(k\)-means (Lloyd's) algorithm.
Clustering was chosen as it is a well-understood problem that is easily
accessible \-- especially when restricted to two dimensions. The \(k\)-means
algorithm is an iterative, centroid-based method that aims to minimise the
``inertia'' of the current partition, \(Z = \left\{Z_1, \ldots, Z_k\right\}\),
of some dataset \(X\):

\begin{equation}
    I(Z, X) := \frac{1}{|X|} \sum_{j=1}^{k} \sum_{x \in Z_j} {d(x, z_j)}^2
\end{equation}

A full statement of the algorithm is given in~\ref{appendix:kmeans}. However, it
should be clear that \(I\) may take any non-negative value.

This inertia function is often taken as the objective of the \(k\)-means
algorithm, and is used for evaluating the final clustering. This is particularly
true when the algorithm is not being considered an unsupervised classifier where
accuracy may be used~\cite{Huang1998}. With that, the first example is to use
this inertia as the fitness function in EDO.\ That is, to find datasets which
minimise \(I\).

In this example, EDO is restricted to only two-dimensional datasets, i.e.\ \(C =
\left((2, 2)\right)\). In addition to this, all columns are formed from the
uniform distribution restricted to the unit interval, \(\mathcal{U} :=
\left\{U(a, b)~|~a, b \in [0, 1]\right\}\). The remaining parameters are as
follows: \(N~=~100\), \(R~=~(3, 100)\), \(M~=~1000\), \(b~=~0.2\), \(l~=~0\),
\(p_m~=~0.01\), and shrinkage excluded. Figure~\ref{figure:inertia-50} shows an
example of the fitness (above) and dimension (below) progression of the
evolutionary algorithm under these conditions up until the \(50^{th}\) epoch.

There is a steep learning curve here; within the first 50 generations an
individual is found with a fitness of roughly \(10^{-10}\) which could not be
improved on for a further 900 epochs. The same quick convergence is seen in the
number of rows. This behaviour is quickly recognised as preferable and was
dominant across all the trials conducted in this work. This preference for
datasets with fewer rows makes sense given that \(I\) is a summation of
non-negative terms (since \(d\) is a distance metric.) With that, when \(k\) is
fixed \textit{a priori}, reducing the number of terms in the second summation
quickly reduces the value of \(I\). 

\begin{figure}[htbp]
    \ContinuedFloat*
    \centering
    \begin{tabular}{c}
        \includegraphics[width=\imgwidth]{img/inertia-fitness-50.pdf}\\
        \includegraphics[width=\imgwidth]{img/inertia-nrows-50.pdf}
    \end{tabular}
    \caption{%
        \label{figure:inertia-50}
        Progressions for final inertia and dimension across the first 50
        epochs with \(R~=~(3,100)\).
    }
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat%
    \centering
    \begin{tabular}{c}
        \includegraphics[width=\imgwidth]{img/large-inertia-fitness-50.pdf}\\
        \includegraphics[width=\imgwidth]{img/large-inertia-nrows-50.pdf}
    \end{tabular}
    \caption{%
        \label{figure:large-inertia-50}
        Progressions for final inertia and dimension across the first 50 epochs
        with \(R~=~(50,100)\).
    }
\end{figure}

Something that may be seen as unwanted is a compaction of the clusters.
Referring to Figure~\ref{figure:inertia-individuals}, the best individual
shows
two clusters but they are all essentially the same point. This kind of behaviour
is exhibited in part because it is allowed. There are two immediate ways in
which this allowed: first, that the ``trivial'' case is included in \(R\) and,
secondly, that the fitness function does nothing to penalise the reduction in
the inter-cluster means, as well as the intra-cluster means. This kind of
unwanted behaviour highlights a subtlety in how EDO should be used;
experimentation and rigour are required to properly understand an algorithm's
quality.

\begin{figure}[htbp]
    \ContinuedFloat*
    \centering
    \includegraphics[width=\imgwidth]{img/inertia-individuals-0.pdf}
    \caption{%
        \label{figure:inertia-individuals}
        Representative individuals based on inertia with \(R~=~(3,100)\).
        Centroids displayed as crosses.
    }
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat%
    \centering
    \includegraphics[width=\imgwidth]{img/large-inertia-individuals-0.pdf}
    \caption{%
        \label{figure:large-inertia-individuals}
        Representative individuals based on inertia with \(R~=~(50,100)\).
        Centroids displayed as crosses.
    }
\end{figure}

Consider Figure~\ref{figure:large-inertia-individuals} where the individuals
have been generated with the same parameters as previously except with adjusted
row limits: \(R = (50, 100)\). In this case, the clusters are still dense about
a single point despite the trivial case being removed from consideration.
Perhaps then, this compact clustering is ``optimal''. However, more extensive
studying may be done. That is, the defined fitness function may require further
attention.

Indeed, the final inertia could be considered a flawed or fragile fitness
function if it is supposed to evaluate the appropriateness or efficacy of the
\(k\)-means algorithm. Incorporating the inter-cluster spread to the fitness of
an individual dataset can reduce this observed compaction. The silhouette
coefficient is a metric used to evaluate the appropriateness of a clustering to
a dataset, and is given by the mean of the silhouette value, \(S(x)\), of each
point \(x \in Z_j\) in each cluster:

\begin{equation}
    \begin{gathered}
        A(x) := \frac{1}{|Z_j| - 1} \sum_{y \in Z_j \setminus \{x\}} d(x, y),
        \qquad B(x) := \min_{k \neq j} \frac{1}{|Z_k|} \sum_{w \in Z_k} d(x, w)
        \\\\
        S(x) := 
            \begin{cases}
                \frac{B(x) - A(x)}{\max\left\{A(x), B(x)\right\}}
                &\quad \text{if } |Z_j| > 1\\
                0 &\quad \text{otherwise}
            \end{cases}\label{eq:silhouette}
    \end{gathered}
\end{equation}\\

The optimisation of the silhouette coefficient is analogous to finding a dataset
which increases both the intra-cluster cohesion (the inverse of \(A\)) and
inter-cluster separation (\(B\)). Hence, the inertia is addressed by maximising
cohesion. Meanwhile, the spread of the clusters themselves is considered by
maximising separation.

Repeating the trials with the same parameters as previously, the silhouette
fitness function yields the results summarised in
Figure~\ref{figure:silhouette}. In this case, the datasets produced have reduced
overlap with one another whilst maintaining low values in the final inertia of
the clustering as shown in Figure~\ref{figure:silhouette-individuals}. Again,
the form of the individual clusters is much the same. The low values of inertia
correspond to tight clusters, and the tightest clusters are those with a minimal
number of points, i.e.\ a single point. As with the previous example, albeit at
a much slower rate, the preferable individuals are those leading toward this
case. That this gradual reduction in the dimension of the individuals occurs
after the improvement of the fitness function bolsters the claim that the base
case is also optimal.

However, due to the nature of the implementation, any individual from any
generation may be retrieved and studied should the final results be too
concentrated on the base case. This transparency in the history and progression
of the proposed method is something that sets it apart from other methods of the
same ilk such as GANs which have a reputation of providing so-called ``black
box'' solutions.

\begin{figure}[htbp]
    \ContinuedFloat*
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/silhouette-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/silhouette-nrows.pdf}
    \end{minipage}
    \caption{Progression for silhouette and dimension across 1000 epochs at 100
             epoch intervals with \(R~=~(3,100)\).}\label{figure:silhouette}
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat%
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/large-silhouette-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/large-silhouette-nrows.pdf}
    \end{minipage}
    \caption{Progression for silhouette and dimension across 1000 epochs at 100
             epoch intervals with
             \(R~=~(50,100)\).}\label{figure:large-silhouette}
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat*
    \centering
    \includegraphics[width=\imgwidth]{img/silhouette-individuals-0.pdf}
    \caption{Representative individuals based on silhouette with
             \(R~=~(3,100)\). Centroids displayed as
             crosses.}\label{figure:silhouette-individuals}
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat%
    \centering
    \includegraphics[width=\imgwidth]{img/large-silhouette-individuals-0.pdf}
    \caption{Representative individuals based on silhouette with
             \(R~=~(50,100)\). Centroids displayed as
             crosses.}\label{figure:large-silhouette-individuals}
\end{figure}


%TODO Include example with large number of rows to discuss Voronoi equivalence?

\subsection{Comparison with DBSCAN}

The capabilities of EDO as a tool for understanding an algorithm are highlighted
particularly when comparing an algorithm against another (or set of others)
simultaneously.  This is done by utilising the freedom of choice in a fitness
function for EDO.\ Consider two algorithms, \(A\) and \(B\), and some common
metric between them, \(g\). Then understanding their similarities and contrasts
can be done by considering the differences in this metric on the two algorithms.
In terms of EDO, this means using \(f = g_A - g_B\), \(f = g_B - g_A\) or \(f
= \left| g_B - g_A \right|\) as the fitness function. By doing so, pitfalls,
edge cases or fundamental conditions for the method can be highlighted.
Overall, this process allows the researcher to more deeply learn about the
method of interest.

As an example of this process, consider the another clustering algorithm of a
different form such as Density Based Spatial Clustering of Applications with
Noise (DBSCAN) and suppose the objective is to find datasets for which
\(k\)-means outperforms this alternative. Here there is no concept of inertia as
DBSCAN is density-based and is able to identify outliers~\cite{Ester1996}. As
such, a valid (and arguably more appropriate) metric is the silhouette score as
defined in~(\ref{eq:silhouette}).

However, an adjustment to the fitness function must be made so as to accommodate
for the condition of the silhouette coefficient that there be more than one
cluster present. Let \(S_k (X)\) and \(S_D (X)\) denote the silhouette
coefficients of the clustering found by \(k\)-means and DBSCAN respectively.
Then the fitness function is defined to be:
\[
    f(X) = 
        \begin{cases}
            S_D (X) - S_k (X), &\quad \text{if DBSCAN identifies two or more
            clusters (including noise)}\\
            \infty &\quad \text{otherwise.}
        \end{cases}
\]

Note the order of the subtraction here as EDO minimises fitness functions by
default.

It must also be acknowledged that \(k\)-means and DBSCAN share no common
parameters and so direct comparison is more difficult. For the purposes of this
example, only one set of parameters is used but a thorough investigation should
include a parameter sweep in cases such as these. The parameters being used are
\(k~=~3\) for \(k\)-means, and \(\epsilon~=~0.1,\ \text{MinPoints}~=~5\) for
DBSCAN.\ With respect to EDO, the parameters are the same as previous with
\(R~=~(50, 100)\).

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/dbscan-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/dbscan-nrows.pdf}
    \end{minipage}
    \caption{Progressions for difference in silhouette (\(k\)-means-preferable)
             and dimension for a run of EDO shown at 100 epoch
             intervals.}\label{figure:dbscan-silhouette}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\imgwidth]{img/kmeans-individuals-0.pdf}

    \includegraphics[width=\imgwidth]{img/dbscan-individuals-0.pdf}
    \caption{Representative individuals with clusters shown. Convex hull shown
             as outline, and approximate concave hull shaded. Above by
             \(k\)-means, below by DBSCAN (noise points shown with small
             markers).}\label{figure:dbscan-individuals}
\end{figure}


\begin{figure}[htbp]
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/negative-dbscan-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/negative-dbscan-nrows.pdf}
    \end{minipage}
    \caption{Progressions for difference in silhouette (DBSCAN-preferable) and
             dimension for a run of EDO shown at 100 epoch
             intervals.}\label{figure:negative-dbscan-silhouette}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\imgwidth]{img/negative-kmeans-individuals-0.pdf}

    \includegraphics[width=\imgwidth]{img/negative-dbscan-individuals-0.pdf}
    \caption{Representative individuals with clusters shown. Convex hull shown
             as outline, and approximate concave hull shaded. Above by
             \(k\)-means, below by DBSCAN (noise points shown with small
             markers).}\label{figure:negative-dbscan-individuals}
\end{figure}
