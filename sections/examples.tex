\section{Examples}\label{section:examples}

\subsection{\(k\)-means clustering}

The following examples act as a form of validation for EDO, and also highlight
some of the nuances in its use. The examples will be focused around the
clustering of data and, in particular, the \(k\)-means (Lloyd's) algorithm.
Clustering was chosen as it is a well-understood problem that is easily
accessible \-- especially when restricted to two dimensions. The \(k\)-means
algorithm is an iterative, centroid-based method that aims to minimise the
``inertia'' of the current partition, \(Z = \left\{Z_1, \ldots, Z_k\right\}\),
of some dataset \(X\):

\begin{equation}
    I(Z, X) := \frac{1}{|X|} \sum_{j=1}^{k} \sum_{x \in Z_j} {d(x, z_j)}^2
\end{equation}

A full statement of the algorithm is given in~\ref{appendix:kmeans}. However, it
should be clear that \(I\) may take any non-negative value.

This inertia function is often taken as the objective of the \(k\)-means
algorithm, and is used for evaluating the final clustering. This is particularly
true when the algorithm is not being considered an unsupervised classifier where
accuracy may be used~\cite{Huang1998}. With that, the first example is to use
this inertia as the fitness function in EDO.\ That is, to find datasets which
minimise \(I\).

In this example, EDO is restricted to only two-dimensional datasets, i.e.\ \(C =
\left((2, 2)\right)\). In addition to this, all columns are formed from the
uniform distribution restricted to the unit interval, \(\mathcal{U} :=
\left\{U(a, b)~|~a, b \in [0, 1]\right\}\). The remaining parameters are as
follows: \(N~=~100\), \(R~=~(3, 100)\), \(M~=~1000\), \(b~=~0.2\), \(l~=~0\),
\(p_m~=~0.01\), and shrinkage excluded. Figure~\ref{figure:inertia-50} shows an
example of the fitness (above) and dimension (below) progression of the
evolutionary algorithm under these conditions up until the \(50^{th}\) epoch.

There is a steep learning curve here; within the first 50 generations an
individual is found with a fitness of roughly \(10^{-10}\) which could not be
improved on for a further 900 epochs. The same quick convergence is seen in the
number of rows. This behaviour is quickly recognised as preferable and was
dominant across all the trials conducted in this work. This preference for
datasets with fewer rows makes sense given that \(I\) is a summation of
non-negative terms (since \(d\) is a distance metric.) With that, when \(k\) is
fixed \textit{a priori}, reducing the number of terms in the second summation
quickly reduces the value of \(I\). 

\begin{figure}[htbp]
    \ContinuedFloat*
    \centering
    \begin{tabular}{c}
        \includegraphics[width=\imgwidth]{img/inertia-fitness-50.pdf}\\
        \includegraphics[width=\imgwidth]{img/inertia-nrows-50.pdf}
    \end{tabular}
    \caption{%
        \label{figure:inertia-50}
        Progressions for final inertia and dimension across the first 50
        epochs with \(R~=~(3,100)\).
    }
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat%
    \centering
    \begin{tabular}{c}
        \includegraphics[width=\imgwidth]{img/large-inertia-fitness-50.pdf}\\
        \includegraphics[width=\imgwidth]{img/large-inertia-nrows-50.pdf}
    \end{tabular}
    \caption{%
        \label{figure:large-inertia-50}
        Progressions for final inertia and dimension across the first 50 epochs
        with \(R~=~(50,100)\).
    }
\end{figure}

Something that may be seen as unwanted is a compaction of the clusters.
Referring to Figure~\ref{figure:inertia-individuals}, the best individual shows
two clusters but they are all essentially the same point whereas the worst is a
random cloud across the whole of \(\mathcal{U}\) which was found in the
initial population. The kind of behaviour exhibited by the best performing
individuals occurs in part because it is allowed. There are two immediate ways
in which this allowed: first, that the ``trivial'' case is included in \(R\)
and, secondly, that the fitness function does nothing to penalise the reduction
in the inter-cluster means, as well as the intra-cluster means. This kind of
unwanted behaviour highlights a subtlety in how EDO should be used;
experimentation and rigour are required to properly understand an algorithm's
quality.

\begin{figure}[htbp]
    \ContinuedFloat*
    \centering
    \includegraphics[width=\imgwidth]{img/inertia-individuals-0.pdf}
    \caption{%
        \label{figure:inertia-individuals}
        Representative individuals based on inertia with \(R~=~(3,100)\).
        Centroids displayed as crosses.
    }
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat%
    \centering
    \includegraphics[width=\imgwidth]{img/large-inertia-individuals-0.pdf}
    \caption{%
        \label{figure:large-inertia-individuals}
        Representative individuals based on inertia with \(R~=~(50,100)\).
        Centroids displayed as crosses.
    }
\end{figure}

Consider Figure~\ref{figure:large-inertia-individuals} where the individuals
have been generated with the same parameters as previously except with adjusted
row limits: \(R = (50, 100)\). In this case, the worst individuals are
equivalent, and the best-performing clusters are still dense about a single
point despite the trivial case being removed from consideration. Perhaps then,
this compact clustering is ``optimal''. However, more extensive studying may be
done. That is, the defined fitness function may require further attention.

Indeed, the final inertia could be considered a flawed or fragile fitness
function if it is supposed to evaluate the appropriateness or efficacy of the
\(k\)-means algorithm. Incorporating the inter-cluster spread to the fitness of
an individual dataset can reduce this observed compaction. The silhouette
coefficient is a metric used to evaluate the appropriateness of a clustering to
a dataset, and is given by the mean of the silhouette value, \(S(x)\), of each
point \(x \in Z_j\) in each cluster:

\begin{equation}
    \begin{gathered}
        A(x) := \frac{1}{|Z_j| - 1} \sum_{y \in Z_j \setminus \{x\}} d(x, y),
        \qquad B(x) := \min_{k \neq j} \frac{1}{|Z_k|} \sum_{w \in Z_k} d(x, w)
        \\\\
        S(x) := 
            \begin{cases}
                \frac{B(x) - A(x)}{\max\left\{A(x), B(x)\right\}}
                &\quad \text{if } |Z_j| > 1\\
                0 &\quad \text{otherwise}
            \end{cases}\label{eq:silhouette}
    \end{gathered}
\end{equation}\\

The optimisation of the silhouette coefficient is analogous to finding a dataset
which increases both the intra-cluster cohesion (the inverse of \(A\)) and
inter-cluster separation (\(B\)). Hence, the inertia is addressed by maximising
cohesion. Meanwhile, the spread of the clusters themselves is considered by
maximising separation.

Repeating the trials with the same parameters as previously, the silhouette
fitness function yields the results summarised in
Figure~\ref{figure:silhouette}. In this case, the datasets produced have reduced
overlap with one another whilst maintaining low values in the final inertia of
the clustering as shown in Figure~\ref{figure:silhouette-individuals}. Again,
the form of the individual clusters is much the same. The low values of inertia
correspond to tight clusters, and the tightest clusters are those with a minimal
number of points, i.e.\ a single point. As with the previous example, albeit at
a much slower rate, the preferable individuals are those leading toward this
case. That this gradual reduction in the dimension of the individuals occurs
after the improvement of the fitness function bolsters the claim that the base
case is also optimal.

However, due to the nature of the implementation, any individual from any
generation may be retrieved and studied should the final results be too
concentrated on the base case. This transparency in the history and progression
of the proposed method is something that sets it apart from other methods of the
same ilk such as GANs which have a reputation of providing so-called ``black
box'' solutions.

\begin{figure}[htbp]
    \ContinuedFloat*
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/silhouette-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/silhouette-nrows.pdf}
    \end{minipage}
    \caption{Progression for silhouette and dimension across 1000 epochs at 100
             epoch intervals with \(R~=~(3,100)\).}\label{figure:silhouette}
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat%
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/large-silhouette-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/large-silhouette-nrows.pdf}
    \end{minipage}
    \caption{Progression for silhouette and dimension across 1000 epochs at 100
             epoch intervals with
             \(R~=~(50,100)\).}\label{figure:large-silhouette}
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat*
    \centering
    \includegraphics[width=\imgwidth]{img/silhouette-individuals-0.pdf}
    \caption{Representative individuals based on silhouette with
             \(R~=~(3,100)\). Centroids displayed as
             crosses.}\label{figure:silhouette-individuals}
\end{figure}

\begin{figure}[htbp]
    \ContinuedFloat%
    \centering
    \includegraphics[width=\imgwidth]{img/large-silhouette-individuals-0.pdf}
    \caption{Representative individuals based on silhouette with
             \(R~=~(50,100)\). Centroids displayed as
             crosses.}\label{figure:large-silhouette-individuals}
\end{figure}


\subsection{Comparison with DBSCAN}

The capabilities of EDO as a tool for understanding an algorithm are highlighted
particularly when comparing an algorithm against another (or set of others)
simultaneously. This is done by utilising the freedom of choice in a fitness
function for EDO.\ Consider two algorithms, \(A\) and \(B\), and some common
metric between them, \(g\). Then understanding their similarities and contrasts
can be done by considering the differences in this metric on the two algorithms.
In terms of EDO, this means using \(f = g_A - g_B\), \(f = g_B - g_A\) or \(f
= \left| g_B - g_A \right|\) as the fitness function. By doing so, pitfalls,
edge cases or fundamental conditions for the method can be highlighted.
Overall, this process allows the researcher to more deeply learn about the
method of interest.

As an example of this process, consider the another clustering algorithm of a
different form such as Density Based Spatial Clustering of Applications with
Noise (DBSCAN) and suppose the objective is to find datasets for which
\(k\)-means outperforms this alternative. Here there is no concept of inertia as
DBSCAN is density-based and is able to identify outliers~\cite{Ester1996}. As
such, a valid (and arguably more appropriate) metric is the silhouette score as
defined in~(\ref{eq:silhouette}).

However, an adjustment to the fitness function must be made so as to accommodate
for the condition of the silhouette coefficient that there be more than one
cluster present. Let \(S_k (X)\) and \(S_D (X)\) denote the silhouette
coefficients of the clustering found by \(k\)-means and DBSCAN respectively.
Then the fitness function is defined to be:
\begin{equation}
    f(X) = 
        \begin{cases}
            S_D (X) - S_k (X), &\quad \text{if DBSCAN identifies two or more
            clusters (including noise)}\\
            \infty &\quad \text{otherwise.}
        \end{cases}\label{equation:dbscan-fitness}
\end{equation}

There are two remarks here. First, note the order of the subtraction here as EDO
minimises fitness functions by default. Also, \(f\) takes values in the range
\([-2, 2]\) where \(-2\) is the best, i.e.\ \(S_D(X) = -1\) and \(S_k(X) = 1\).
Likewise, 2 is the worst score.

It must also be acknowledged that \(k\)-means and DBSCAN share no common
parameters and so direct comparison is more difficult. For the purposes of this
example, only one set of parameters is used but a thorough investigation should
include a parameter sweep in cases such as these. The parameters being used are
\(k~=~3\) for \(k\)-means, and \(\epsilon~=~0.1,\ MinPoints~=~5\) for DBSCAN.\
This set was chosen following informal experimentation using the Python library
Scikit-learn~\cite{scikit} to find comparable parameters in the given search
space defined by the EDO parameters used previously with \(R~=~(50,100)\).

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/dbscan-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/dbscan-nrows.pdf}
    \end{minipage}
    \caption{Progressions for difference in silhouette (\(k\)-means-preferable)
             and dimension across 1000 epochs at 100 epoch
             intervals.}\label{figure:dbscan-silhouette}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\imgwidth]{img/kmeans-individuals-0.pdf}

    \includegraphics[width=\imgwidth]{img/dbscan-individuals-0.pdf}
    \caption{Representative individuals with clusters shown. Convex hull shown
             as outline, and approximate concave hull shaded. Above by
             \(k\)-means, below by DBSCAN (noise points shown with small
             markers).}\label{figure:dbscan-individuals}
\end{figure}

Figure~\ref{figure:dbscan-silhouette} shows a summary of the progression of EDO
for this use case. As with the previous examples where \(R~=~(50, 100)\), the
variation in the population fitness is unstable but there is a clear trend of
improvement in the best individual over the course of the run. There is also a
convergence seen in the number of rows a dataset has. The resting dimension
varied across the trials conducted in this work but none exhibited a shift
toward the lower limit of 50 rows as with previous examples. This is suggestive
of a more competitive environment for individuals where slight changes to an
individual can drastically alter their fitness.

The effect of such changes can be seen in Figure~\ref{figure:dbscan-individuals}
where representative individuals are shown for this example. Here, the best
performing individual, when clustered by \(k\)-means, shows three clear and
nicely separated clusters. Note that they are not so tightly packed; again, this
suggests that the route to an optimal individual is less clearly defined. In
contrast, when the same dataset is clustered by DBSCAN a single cluster is found
with a single noise point held within the convex hull of the cluster, i.e.\
there are overlapping clusters (since noise points form a single cluster).
Hence, along with the fact that the larger cluster is widely spread, it follows
that the clustering have a relatively small, negative silhouette coefficient.

Another point of interest here is the convexity of the clusters. One of the
conditions for the success of \(k\)-means is that it requires convex clusters as
the objective is to approximate the centroidal Voronoi
tessellation~\cite{Du2006}. Without this condition, up to the correct choice of
\(k\), the algorithm will fail to produce adequate results for either inertia or
silhouette. DBSCAN, however, does not have this condition and is able to detect
non-convex clusters so long as they are dense enough.
Figure~\ref{figure:dbscan-individuals} shows the convex and concave hulls of the
clusters found by each method. The ``concave hull'' of a cluster is taken to be
the \(\alpha\)-shape of the cluster's data points~\cite{Edelsbrunner1983} where
\(\alpha\) is determined to be the smallest value such that all the points in
the cluster are contained in a single polygon. The convexity of cluster \(Z_j\),
denoted \(\mathcal{C}_j\), is then determined to be the ratio of the area of its
concave hull, \(H_c\), to the area of its convex hull, \(H_v\)~\cite{Sonka1993}:

\begin{equation}
    \mathcal{C}_j := \frac{area(H_c)}{area(H_v)}
\end{equation}

With this definition, it should be clear that a perfectly convex cluster would
have \(C_j = 1\).

It can be seen that the convexity of the clustering found by \(k\)-means appears
to be higher than that by DBSCAN.\ This was apparent across all trials conducted
in this work and indicates that the condition for convex clusters is being
sought out by during the optimisation process. Meanwhile, however, it is not
clear whether the performance of DBSCAN falls owing to its parameters or the
method itself. This is a point where parameter sweeping would prove most useful
so as to determine a crossing point for these two driving forces.

% TODO Perhaps some statistical test could be used here, or with the second part
% of this example.

Now, to bolster the discussion above, it is required that the inverse
optimisation be considered. That is, using the same parameters, investigate the
datasets for which DBSCAN outperforms \(k\)-means with respect to the silhouette
coefficient. This is equivalent to using \(-f\) as the fitness function except
with the same penalty of \(\infty\) for the case set out
in~(\ref{equation:dbscan-fitness}).
Figures~\ref{figure:negative-dbscan}~\&~\ref{figure:negative-dbscan-individuals}
show the equivalent summary as above with the revised fitness function.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/negative-dbscan-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/negative-dbscan-nrows.pdf}
    \end{minipage}
    \caption{Progressions for difference in silhouette (DBSCAN-preferable) and
             dimension across 1000 epochs at 100 epoch
             intervals.}\label{figure:negative-dbscan}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\imgwidth]{img/negative-kmeans-individuals-0.pdf}

    \includegraphics[width=\imgwidth]{img/negative-dbscan-individuals-0.pdf}
    \caption{Representative individuals with clusters shown. Convex hull shown
             as outline, and approximate concave hull shaded. Above by
             \(k\)-means, below by DBSCAN (noise points shown with small
             markers).}\label{figure:negative-dbscan-individuals}
\end{figure}
