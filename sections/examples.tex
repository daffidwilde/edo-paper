\section{Examples}\label{section:examples}

\subsection{\(k\)-means clustering}

The following examples act as a form of validation for EDO, and also highlight
some of the nuances in its use. The examples will be focused around the
clustering of data and, in particular, the \(k\)-means (Lloyd's) algorithm.
Clustering was chosen as it is a well-understood problem that is easily
accessible \-- especially when restricted to two dimensions. The \(k\)-means
algorithm is an iterative, centroid-based method that aims to minimise the
``inertia'' of the current partition, \(Z = \left\{Z_1, \ldots, Z_k\right\}\),
of some dataset \(X\):

\begin{equation}
    I(Z, X) := \frac{1}{|X|} \sum_{j=1}^{k} \sum_{x \in Z_j} {d(x, z_j)}^2
\end{equation}

A full statement of the algorithm is given in~\ref{appendix:kmeans}. However, it
should be clear that \(I\) may take any non-negative value.

This inertia function is often taken as the objective of the \(k\)-means
algorithm, and is used for evaluating the final clustering. This is particularly
true when the algorithm is not being considered an unsupervised classifier where
accuracy may be used~\cite{Huang1998}. With that, the first example is to use
this inertia as the fitness function in EDO.\ That is, to find datasets which
minimise \(I\).

In this example, EDO is restricted to only two-dimensional datasets, i.e.\ \(C =
\left((2, 2)\right)\). In addition to this, all columns are formed from the
uniform distribution restricted to the unit interval, \(\mathcal{U} :=
\left\{U(a, b)~|~a, b \in [0, 1]\right\}\). The remaining parameters are as
follows: \(N~=~100\), \(R~=~(3, 100)\), \(M~=~1000\), \(b~=~0.2\), \(l~=~0\),
\(p_m~=~0.01\), and shrinkage excluded. Figure~\ref{figure:inertia-50} shows an
example of the fitness (above) and dimension (below) progression of the
evolutionary algorithm under these conditions up until the \(50^{th}\) epoch.

%\begin{figure}[htbp]
%    \centering
%    \begin{minipage}{\imgwidth}
%        \centering
%        \includegraphics[width=\linewidth]{img/inertia-fitness.pdf}
%    \end{minipage}
%
%    \begin{minipage}{\imgwidth}
%        \centering
%        \includegraphics[width=\linewidth]{img/inertia-nrows.pdf}
%    \end{minipage}
%    \caption{Progressions for fitness and dimension (number of rows) for a run
%             of EDO shown at 100 epoch intervals.}\label{figure:inertia}
%\end{figure}

There is a steep learning curve here; within the first 50 generations an
individual is found with a fitness of roughly \(10^{-10}\) which could not be
improved on for a further 900 epochs. The same quick convergence is seen in the
number of rows. This behaviour is quickly recognised as preferable and was
dominant across all the trials conducted in this work. This preference for
datasets with fewer rows makes sense given that \(I\) is a summation of
non-negative terms (since \(d\) is a distance metric.) With that, when \(k\) is
fixed \textit{a priori}, reducing the number of terms in the second summation
quickly reduces the value of \(I\). 

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/inertia-fitness-50.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/inertia-nrows-50.pdf}
    \end{minipage}
    \caption{Progressions for final inertia and dimension across the first 50
             epochs.}\label{figure:inertia-50}
\end{figure}

Something that may be seen as unwanted is a compaction of the clusters.
Referring to Figure~\ref{figure:inertia-individuals}, the best individual shows
two clusters but they are all essentially the same point. This kind of behaviour
is exhibited in part because it is allowed. That is, the fitness function used
with EDO does nothing to penalise the reduction in the inter-cluster means, as
well as the intra-cluster means. This kind of unwanted behaviour highlights a
subtlety in how EDO is used, namely that the definition of the fitness function
requires attention and consideration as the outcome of the EA is entirely
dependent on it.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\imgwidth]{img/inertia-individuals-0.pdf}
    \caption{Representative individuals from the final generation based on
             inertia. Centroids displayed as
             crosses.}\label{figure:inertia-individuals}
\end{figure}

Indeed, this fitness function could be considered flawed or fragile if it is
supposed to evaluate the appropriateness or efficacy of the \(k\)-means
algorithm. One additional consideration in the fitness of an individual dataset
can reduce this observed compaction effect. The silhouette coefficient is a
metric used to evaluate the appropriateness of a clustering to a dataset, and is
given by the mean of the silhouette value, \(S(x)\), of each point \(x \in Z_j\)
in each cluster:

\[
    A(x) := \frac{1}{|Z_j| - 1} \sum_{y \in Z_j \setminus \{x\}} d(x, y),
    \qquad B(x) := \min_{k \neq j} \frac{1}{|Z_k|} \sum_{w \in Z_k} d(x, w)
\]

\begin{equation}
        S(x) := 
            \begin{cases}
                \frac{B(x) - A(x)}{\max\left\{A(x), B(x)\right\}}
                &\quad \text{if } |Z_j| > 1\\
                0 &\quad \text{otherwise}
            \end{cases}\label{eq:silhouette}
\end{equation}\\

The optimisation of the silhouette coefficient is analogous to finding a dataset
which balances the cohesion (the inverse of \(A\)) and separation (\(B\)) of its
clusters. Hence, the inertia is addressed by maximising cohesion. Meanwhile, the
spread of the clusters themselves is considered by maximising separation.

Repeating the trials with the same parameters as previously, the silhouette
fitness function yields the results summarised in
Figure~\ref{figure:silhouette}. In this case, the datasets produced have reduced
overlap with one another whilst maintaining low values in the final inertia of
the clustering as shown in Figure~\ref{figure:silhouette-individuals}. Again,
the form of the individual clusters is much the same. The low values of inertia
correspond to tight clusters, and the tightest clusters are those with a minimal
number of points, i.e.\ a single point. As with the previous example, albeit at
a much slower rate, the preferable individuals are those leading toward this
case. That this gradual reduction in the dimension of the individuals occurs
after the improvement of the fitness function bolsters the claim that the base
case is also optimal.

However, due to the nature of the implementation, any individual from any
generation may be retrieved and studied should the final results be too
concentrated on the base case. This transparency in the history and progression
of the proposed method is something that sets it apart from other methods of the
same ilk such as GANs which have a reputation of providing so-called ``black
box'' solutions.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/silhouette-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/silhouette-nrows.pdf}
    \end{minipage}
    \caption{Progressions for the silhouette and dimension (number of rows) for
             a run of EDO shown at 100 epoch
             intervals.}\label{figure:silhouette}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\imgwidth]{img/silhouette-individuals-0.pdf}
    \caption{Representative individuals from the final generation based on
             silhouette. Centroids displayed as
             crosses.}\label{figure:silhouette-individuals}
\end{figure}

%TODO Include example with large number of rows to discuss Voronoi equivalence?

\subsection{Comparison with DBSCAN}

The capabilities of EDO as a tool for understanding an algorithm are highlighted
particularly when comparing an algorithm against another (or set of others)
simultaneously.  This is done by utilising the freedom of choice in a fitness
function for EDO.\ Consider two algorithms, \(A\) and \(B\), and some common
metric between them, \(g\). Then understanding their similarities and contrasts
can be done by considering the differences in this metric on the two algorithms.
In terms of EDO, this means using \(f = g_A - g_B\), \(f = g_B - g_A\) or \(f
= \left| g_B - g_A \right|\) as the fitness function. By doing so, pitfalls,
edge cases or fundamental conditions for the method can be highlighted.
Overall, this process allows the researcher to more deeply learn about the
method of interest.

As an example of this process, consider the another clustering algorithm of a
different form such as Density Based Spatial Clustering of Applications with
Noise (DBSCAN) and suppose the objective is to find datasets for which
\(k\)-means outperforms this alternative. Here there is no concept of inertia as
DBSCAN is density-based and is able to identify outliers~\cite{Ester1996}. As
such, a valid (and arguably more appropriate) metric is the silhouette score as
defined in~(\ref{eq:silhouette}).

However, an adjustment to the fitness function must be made so as to accommodate
for the condition of the silhouette coefficient that there be more than one
cluster present. Let \(S_k (X)\) and \(S_D (X)\) denote the silhouette
coefficients of the clustering found by \(k\)-means and DBSCAN respectively.
Then the fitness function is defined to be:
\[
    f(X) = 
        \begin{cases}
            S_D (X) - S_k (X), &\quad \text{if DBSCAN identifies two or more
            clusters}\\
            \infty &\quad \text{otherwise.}
        \end{cases}
\]

Note the order of the subtraction here as EDO minimises fitness functions by
default.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/dbscan-fitness.pdf}
    \end{minipage}

    \begin{minipage}{\imgwidth}
        \centering
        \includegraphics[width=\linewidth]{img/dbscan-nrows.pdf}
    \end{minipage}
    \caption{Progressions for difference in silhouette and dimension for a run
             of EDO shown at 100 epoch
             intervals.}\label{figure:dbscan-silhouette}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\imgwidth]{img/kmeans-individuals-0.pdf}

    \includegraphics[width=\imgwidth]{img/dbscan-individuals-0.pdf}
    \caption{Representative individuals from the final generation with
             clusters shown. Convex hull shown as outline, and approximate
             concave hull shaded. Above by \(k\)-means, below by
             DBSCAN.}\label{figure:dbscan-individuals}
\end{figure}

