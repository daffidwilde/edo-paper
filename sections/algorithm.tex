\section{The evolutionary algorithm}\label{section:algorithm}

\subsection{Structure}

In this section, the details of an algorithm that generates data for which a
given function or, equivalently, an algorithm which is well suited, is
described. This algorithm is to be referred to as ``Evolutionary Dataset
Optimisation'' (EDO).

The EDO method is built as an evolutionary algorithm which follows a traditional
(generic) schema with some additional features that keep the objective of
artificial data generation in mind. With that, there are a number of parameters
that are passed to EDO;\ the typical parameters of an evolutionary algorithm
are a fitness function, \(f\), which maps from an individual to a real number,
as well as a population size, \(N\), a maximum number of iterations, \(M\), a
selection parameter, \(b\), and a  mutation probability, \(p_m\). In addition to
these, EDO takes the following parameters:

\begin{itemize}
    \item Limits on the number of rows an individual dataset can have:
        \[
            R \in \left\{%
                (r_{\min}, r_{\max}) \in \mathbb{N}^2~|~r_{\min} \leq r_{\max}
            \right\}
        \]
    \item Limits on the number of columns a dataset can have:
        \[
            C := \left(C_1, \ldots, C_{|\mathcal{P}|}\right)
            ~\text{where}~
            C_j \in \left\{ (c_{\min}, c_{\max}) \in {%
                \left(\mathbb{N}\cup\{\infty\}\right)
            }^2~|~c_{\min} \leq c_{\max}\right\}
        \]
        for each \(j = 1, \ldots, |\mathcal{P}|\). That is, \(C\) defines the
        minimum and maximum number of columns a dataset may have from each
        distribution in \(\mathcal{P}\).
    \item A set of probability distribution families, \(\mathcal{P}\). Each
        family in this set has some parameter limits which form a part of the
        overall search space. For instance, the family of normal distributions,
        denoted by \(N(\mu, \sigma^2)\), would have limits on values for the
        mean, \(\mu\), and the standard deviation, \(\sigma\).
    \item A probability vector to sample distributions from \(\mathcal{P}\),
        \(w = \left(w_1, \ldots, w_{|\mathcal{P}|}\right)\).
    \item A second selection parameter, \(l \in [0, 1]\), to allow for a
        small proportion of ``lucky'' individuals to be carried forward.
    \item A shrink factor, \(s \in [0, 1]\), defining the relative size of a
        component of the search space to be retained after adjustment.
\end{itemize}

The concepts discussed in this section form the mechanisms of the evolutionary
dataset optimisation algorithm. To use the algorithm practically, these
components have been implemented in Python as a library built on the scientific
Python stack~\cite{pandas,numpy}. The library is fully tested and documented (at
\url{https://edo.readthedocs.io}) and is freely available online under the MIT
license~\cite{edo-project}. The EDO implementation was developed to be
consistent with the best practices of open source software development.
% TODO Citation(s) needed.

\input{tex/algorithms/edo.tex}\label{alg:edo}
\input{tex/algorithms/new_population.tex}

The statement of the EDO algorithm is presented here to lay out its general
structure from a high level perspective. Lower level discussion is provided
below where additional algorithms for the individual creation, evolutionary
operator and shrinkage processes are given along with diagrams (where
appropriate).

Note that there are no defined processes for how to stop the algorithm or adjust
the mutation probability, \(p_m\). This is down to their relevance to a
particular use case. Some examples include:

\begin{itemize}
    \item Stopping when no improvement in the best fitness is found within some
        \(K\) consecutive iterations~\cite{Leung2001}.
    \item Utilising global behaviours in fitness to indicate a stopping
        point~\cite{Marti2016}.
    \item Regular decreasing in mutation probability across the available
        attributes~\cite{Kuehn2013}.
\end{itemize}

\subsection{Individuals}

Evolutionary algorithms operate in an iterative process on populations of
individuals that each represent a solution to the problem in question. In a
genetic algorithm, an individual is a solution encoded as a bit string of,
typically, fixed length and treated as a chromosome-like object to be
manipulated. In EDO, as the objective is to generate datasets and explore the
space in which datasets exist, there is no encoding. As such the distinction is
made that EDO is an evolutionary algorithm. Instead of this encoding, the
produced datasets are manipulated directly so that the biological operators can
be designed and be interpreted in a more meaningful way.

As is seen in Figure~\ref{figure:individual}, an individual's creation
is defined by the generation of its columns. A set of instructions on how to
sample new values for that column are recorded in the form of a probability
distribution. These distributions are sampled and created from the families
passed in \(\mathcal{P}\).

\inputtikz{individual}{%
    An example of how an individual is first created.
}

A caveat to this: one should not assume that the columns are a reliable
representative of the distribution associated with them, or vice versa. This is
particularly true of ``shorter'' datasets with a small number of rows, whereas
confidence in the pair could be given more liberally for ``longer'' datasets
with a larger number of rows. In any case, appropriate methods should be
employed to understand the structure and characteristics of the data produced
before formal conclusions are made.

\input{tex/algorithms/individual.tex}

\subsection{Selection}

The selection operator describes the process by which individuals are chosen
from the current population to generate the next. Almost always, the likelihood
of an individual being selected is determined by their fitness. This is because
the purpose of selection is to preserve favourable qualities and encourage some
homogeneity within future generations~\cite{Back1994}.

\inputtikz[.8\textwidth]{selection}{%
    The selection process with the inclusion of some lucky individuals.
}
\input{tex/algorithms/selection.tex}

In EDO, a modified truncation selection method is used, as can be seen in
Figure~\ref{figure:selection}. Truncation selection takes a fixed number, \(n_b
= \lceil bN\rceil\), of the fittest individuals in a population and makes them
the ``parents'' of the next. The modification is an optional stage after the
best individuals have been chosen: by passing some small \(l\) to the
evolutionary algorithm, a number of the remaining individuals can be selected at
random to be carried forward. This number is given by \(n_l = \lceil lN
\rceil\). It should be noted that even with this modification, no individual may
be selected more than once in a single iteration but could potentially be
present throughout the run of the algorithm.

It has been observed that, despite its efficiency as a selection operator,
truncation selection can lead to premature convergence at local
optima~\cite{Jebari2013, Tatsuya2002}. Hence, allowing for the inclusion of a
small number of randomly selected individuals may encourage diversity and
exploration throughout the run of the algorithm.

A final component to this part of the evolutionary algorithm provides the
ability to ``shrink'' the search space about the region observed in a given
population. This method is based on a power law described
in~\cite{Amirjanov2016} that relies on a shrink factor, \(s\). At each
iteration, \(t\), the parents are taken and every present distribution's
parameter limits, \(\left(l_t, u_t\right)\), are centred about the mean parent
value, \(\mu\):
\begin{align}
    \label{eq:shrinking_lower}
    l_{t+1}&= \max \left\{l_t, \ \mu - \frac{1}{2} (u_t - l_t) s^t\right\}\\
    \label{eq:shrinking_upper}
    u_{t+1}&= \max \left\{u_t, \ \mu + \frac{1}{2} (u_t - l_t) s^t\right\}
\end{align}

This shrinking process is described in Algorithm~\ref{algorithm:shrinking} but
note that the behaviour of this process can give reductive results for some use
cases. For instance, where there exist multiple optima for dataset
characteristics.

\input{tex/algorithms/shrinking.tex}

\subsection{Crossover}

Crossover is the operation of combining two individuals in order to create at
least one offspring. In EDO, a method known as uniform crossover is
used~\cite{Semenkin2012}. Under this method, a new individual is created by
uniformly sampling each of its
components from a set of two ``parent'' individuals. As can be seen in
Figure~\ref{figure:crossover}, this method has been adapted for the dataset
representation, i.e.\ two parent datasets have their dimensions and then columns
sampled uniformly and without replacement to give a new individual.

\inputtikz{crossover}{The crossover process.}
\input{tex/algorithms/crossover.tex}

\subsection{Mutation}

Mutation is used in evolutionary algorithms to encourage a broader exploration
of the search space at each generation. Under this framework, the mutation
process manipulates the phenotype of an individual where numerous things need to
be modified including an individual's dimensions, column metadata and the
entries themselves. This process is described in Figure~\ref{figure:mutation}.

\inputtikz{mutation}{The mutation process.}

As shown in Figure~\ref{figure:mutation}, each of the potential mutations occur
with the same probability, \(p_m\). However, the way in which columns are
maintained assure that (assuming appropriate choices for \(f\) and
\(\mathcal{P}\)) many mutations in the metadata and the dataset itself will only
result in some incremental change in the individual's fitness
relative to, say, a completely new individual.

\input{tex/algorithms/mutation.tex}
