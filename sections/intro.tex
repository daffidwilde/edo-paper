\section{Introduction}\label{section:introduction}

When an algorithm is developed to solve a given problem, the developer is
presented with questions about the performance of their proposed method, and its
relative performance against existing methods. The standard response to these
situations is to fix some benchmark datasets and a common metric amongst the
proposed method and its competitors. The algorithm is then assessed based on
this metric. This paradigm has a number of problems:

\begin{enumerate}
    \item How were these benchmark examples selected? In some domains and
        disciplines there are well-established benchmarks so those found through
        literature can be reliable, in others less so.
    \item Sometimes, when there is a lack of benchmark examples, a `new' dataset
        is simulated to assess the algorithm. This begs the question as to how
        and why that simulation is created.
    \item In disciplines where there are established benchmarks, there may still
        be underlying problems:
        \begin{enumerate}[(i)]
            \item As an example, work by Torralba and Efros~\cite{Torralba2011}
                showed that image classifiers trained and evaluated on a
                particular dataset, or datasets, did not perform reliably when
                evaluated using other benchmark datasets.
            \item The amount of learning one can gain as to the characteristics
                of data which lead to good (or bad) performance of an algorithm
                is constrained to the finite set of attributes present in the
                benchmark data chosen in the first place.
        \end{enumerate}
\end{enumerate}

This work presents a novel approach that flips this paradigm on its head.
Instead of fixing data and evaluating an algorithm on some metric, the proposed
method will generate data for which the algorithm performs well (or poorly)
through evolution. These datasets can then be studied to learn which attributes
and characteristics lead to the success (or failure) of a given algorithm.

% TODO: Diagram showing this flipped paradigm.

Evolutionary algorithms (EAs) have been applied successfully to solve a wide
array of problems in recent history \-- particularly where the complexity of the
problem or its domain are significant. These methods are highly adaptive and
their population-based construction allows for the efficient solving of problems
that are otherwise beyond the scope of traditional search and optimisation
methods. It should be clear that the study of algorithm quality falls into this
category of problems.

\inputtikz{flowchart}{%
    A general schematic for an evolutionary algorithm.
}

The use of EAs to generate artificial data, however, is not a new concept; these
methods have been used to develop techniques for the automated testing of
software with a goal of achieve maximal coverage or to support structural
tests~\cite{Koleejan2015,Michael2001,Sharifipour2018}. Such methods also have
a long history in the parameter optimisation of algorithms, and recently in the
automated design of convolutional neural network
architecture~\cite{Suganuma2017,Sun2018}. 
