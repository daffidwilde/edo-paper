\section{Introduction}\label{section:introduction}

This work presents a novel approach to learning the quality and performance of
an algorithm through the use of evolution. When an algorithm is developed to
solve a given problem, the developer is presented with questions about the
performance of their proposed method, and its relative performance against
existing methods. Under the current paradigm, the standard response to this
situation is to fix some datasets and a common metric amongst the proposed
method and its competitors. The algorithm is then assessed based on this metric
with minimal consideration for both the appropriateness or reliability of the
datasets being used, and the robustness of the method in question.

This notion is not so easily observed when travelling in the opposite direction.
Suppose that, instead, the benchmark was a dataset of particular interest and a
preferable algorithm was to be determined. There exist a number of methods
employed across disciplines to complete this task that take into account the
characteristics of the data and the context of the research problem. These
methods include the use of diagnostic tests. For instance, in the case of
clustering, if the data displayed an indeterminate number of non-convex blobs,
then one could recommend that an appropriate clustering algorithm would be
DBSCAN~\cite{Ester1996}. Otherwise, for easy scalability, \(k\)-means may be
chosen.

The approach presented in this work aims to flip the paradigm described here by
allowing the data itself to be unfixed. This fluidity in the data is achieved by
generating data for which the algorithm performs well (or better than some
other) through the use of an evolutionary algorithm. The purpose of doing so is
not to simply create a bank of useful datasets but rather to allow for the
subsequent studying of these datasets. In doing so, the attributes and
characteristics which lead to the success (or failure) of the algorithm may be
described, giving a broader understanding of the algorithm on the whole.

\inputtikz[.9\textwidth]{paradigm}{%
    On the right: the path taken by data scientists currently for selecting some
    algorithm(s) based on their performance for a given dataset. On the left:
    the proposed flip to understand the space in which `good' datasets exist.
}

This proposed flip has a number of motivations, and below is a non-exhaustive
list of some of the problems that are presented by the established evaluation
paradigm:

\begin{enumerate}
    \item How are these benchmark examples selected? There is no true measure of
        their reliability other than their frequent use. In some domains and
        disciplines there are well-established benchmarks so those found through
        literature may well be reliable, but in others less so.
    \item Sometimes, when there is a lack of benchmark examples, a `new' dataset
        is simulated to assess the algorithm. This begs the question as to how
        and why that simulation is created. Not only this, but the origins of
        existing benchmarks is often a matter of convenience rather than their
        merit.
    \item In disciplines where there are established benchmarks, there may still
        be underlying problems around the true performance of an algorithm:
        \begin{enumerate}[(i)]
            \item As an example, work by Torralba and Efros~\cite{Torralba2011}
                showed that image classifiers trained and evaluated on a
                particular dataset, or datasets, did not perform reliably when
                evaluated using other benchmark datasets that were determined
                to be similar. Thus leading to a model which lacks robustness.
            \item The amount of learning one can gain as to the characteristics
                of data which lead to good (or bad) performance of an algorithm
                is constrained to the finite set of attributes present in the
                benchmark data chosen in the first place.
        \end{enumerate}
\end{enumerate}

Evolutionary algorithms (EAs) have been applied successfully to solve a wide
array of problems in recent history \-- particularly where the complexity of the
problem or its domain are significant. These methods are highly adaptive and
their population-based construction (displayed in Figure~\ref{figure:flowchart})
allows for the efficient solving of problems that are otherwise beyond the scope
of traditional search and optimisation methods. It should be clear that the
study of algorithm quality falls into this category of problems.

\inputtikz{flowchart}{%
    A general schematic for an evolutionary algorithm.
}

The use of EAs to generate artificial data, however, is not a new concept; these
methods have been used to develop techniques for the automated testing of
software with a goal of achieve maximal coverage or to support structural
tests~\cite{Koleejan2015,Michael2001,Sharifipour2018}. Such methods also have
a long history in the parameter optimisation of algorithms, and recently in the
automated design of convolutional neural network
architecture~\cite{Suganuma2017,Sun2018}.

% TODO make reference to GANs and Mark Elliot's work.

The unconstrained learning style of methods such as neural networks aligns with
that proposed in this work. By allowing the EA to explore and learn about the
search space in an organic way, a more unprejudiced level of insight can be
established that is free from any particular framework or agenda.
