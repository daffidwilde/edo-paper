\section{Introduction}\label{section:introduction}

When a research problem manifests itself and an algorithm is developed to solve
the problem, the investigator is presented with questions about the performance
of their proposed method, and its relative performance to existing methods. The
standard response to these questions is to fix some benchmark datasets and a
common metric amongst the proposed method and its competitors. The algorithm is
then assessed based on this metric. This paradigm has a number of problems:

\begin{enumerate}
    \item How were these benchmark examples selected?

Evolutionary algorithms (EAs) have been applied successfully to solve a wide
array of problems in recent history \-- particularly where the complexity of the
problem or its domain are significant. These methods are highly adaptive and
their population-based construction allows for the efficient solving of problems
that are otherwise beyond the scope of traditional search and optimisation
methods.

\inputtikz{flowchart}{%
    A general schematic for an evolutionary algorithm.
}

\begin{itemize}
    \item What is the motivation?\\ Here, a diagram might be useful to highlight
        the flipped paradigm. Typically one would fix some benchmark examples
        and a metric, and then assess the algorithm based on this metric. This
        paradigm has a number of problems:\\ 1. How were the benchmark examples
        selected? In some disciplines/domains there are well established
        benchmarks, in others less so. Sometimes a `new' data set is simulated
        to assess the performance of an algorithm. How and why is this
        simulation created?\\ 2. In disciplines where there are established
        benchmarks, there still may be problems e.g. (i) Work by Hyndman, who
        considered benchmark data sets used in time series analysis
        competitions, showed that the data sets were biased towards particular
        attributed; (ii) the amount of learning one can gain as to the
        attributes of data which lead to good (or bad) performance of an
        algorithm are constrained to the finite set of attributes present in the
        benchmark data chosen in the first place.
    \item What is the problem?
    \item What is the solution?
\end{itemize}

\subsection{Literature review}

\begin{itemize}
    \item How is artificial data made?
    \item Why hasn't this been done before?
    \item Genetic algorithms used to train algorithms for data
    \item Diagram showing that this is the ``reverse'' problem
\end{itemize}
